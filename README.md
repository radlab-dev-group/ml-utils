# radlab-ml-utils

## ðŸ“– Overview

`radlab-ml-utils` is a collection of helper utilities and handlers designed to simplify  
common machineâ€‘learning workflows. The package includes:

- **OpenAPI handler** â€“ thin client for LLM servers exposing an OpenAPI spec.
- **Training handler** â€“ utilities for preparing datasets, tokenization and orchestrating training pipelines.
- **WandB handler** â€“ convenient wrappers around Weights & Biases for experiment tracking,
  artifact management and rich logging.
- **Prompt handler** â€“ loads and manages prompt files (`*.prompt`) from a directory tree,
  making it easy to reuse and reference prompts programmatically.

The library is built on Python 3.10 and can be installed via `pip install .` after cloning the repository.

---

## ðŸ“‚ Project Structure

```
radlab-ml-utils/
â”‚
â”œâ”€ apps/
â”‚   â””â”€ __init__.py
â”‚   â””â”€ openapi_test.py          # Example script demonstrating the OpenAPI client
â”‚
â”œâ”€ configs/
â”‚   â””â”€ ollama_config.json       # Sample OpenAPI configuration file
â”‚
â”œâ”€ rdl_ml_utils/
â”‚   â”œâ”€ handlers/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ openapi_handler.py   # Core OpenAPI client implementation
â”‚   â”‚   â”œâ”€ training_handler.py  # Dataset loading & training helpers
â”‚   â”‚   â”œâ”€ wandb_handler.py     # W&B integration utilities
â”‚   â”‚   â””â”€ prompt_handler.py    # Prompt loading and lookup utilities
â”‚   â””â”€ utils/
â”‚       â””â”€ __init__.py
â”‚
â”œâ”€ .gitignore
â”œâ”€ CHANGELOG.md
â”œâ”€ LICENSE
â”œâ”€ README.md                    # *You are reading it right now*
â”œâ”€ requirements.txt
â””â”€ setup.py
```

---

## ðŸ› ï¸ Handlers

### `openapi_handler.py`

The **OpenAPI client** (`OpenAPIClient`) provides a simple, opinionated interface for interacting with LLM servers that
follow the OpenAIâ€‘compatible API schema.

#### Key Features

| Feature                     | Description                                                                                      |
|-----------------------------|--------------------------------------------------------------------------------------------------|
| **Flexible initialization** | Pass `base_url`â€¯/â€¯`model` directly **or** load them from a JSON config file (`open_api_config`). |
| **Authentication**          | Optional `api_key` is added as a `Bearer` token header when supplied.                            |
| **Prompt generation**       | `generate(prompt, â€¦)` returns a plainâ€‘text completion.                                           |
| **Chat completions**        | `chat(messages, â€¦)` works with the standard `[{role, content}]` message format.                  |
| **System prompt handling**  | A global `system_prompt` can be set at client creation and overridden per call.                  |
| **Health check**            | `is_available()` performs a quick GET request to verify server reachability.                     |
| **Contextâ€‘manager**         | Use `with OpenAPIClient(...) as client:` for clean entry/exit semantics.                         |

#### Example Usage

```python
from rdl_ml_utils.handlers.openapi_handler import OpenAPIClient

# Load configuration from JSON (recommended for reproducibility)
with OpenAPIClient(open_api_config="configs/ollama_config.json") as client:
    # Verify the server is up
    if not client.is_available():
        raise RuntimeError("OpenAPI server is not reachable.")

    # Simple generation
    answer = client.generate(
        message="Explain logistic regression.",
        system_prompt="You are a statistics expert.",
        max_tokens=512,
    )
    print("Generation result:", answer)

    # Chatâ€‘style interaction
    chat_messages = [
        {"role": "user", "content": "What are the biggest challenges in ML today?"},
    ]
    response = client.chat(
        messages=chat_messages,
        system_prompt="Speak like a senior data scientist.",
        max_tokens=256,
    )
    print("Chat response:", response)
```

#### Configuration File (`configs/ollama_config.json`)

```json
{
  "base_url": "http://localhost:11434",
  "model": "MODEL_NAME",
  "api_key": "YOUR_API_KEY_IF_NEEDED",
  "system_prompt": "You are a helpful AI assistant."
}
```

---

### `openapi_queue_manager.py`

The **OpenAPI queue manager** (`OpenAPIQueue`) provides a thin, threadâ€‘safe wrapper around multiple
`OpenAPIClient` instances. It loads client configurations from JSON files, creates a pool of worker
threads, and processes `generate` and `chat` requests in a firstâ€‘inâ€‘firstâ€‘out (FIFO) order.

#### Why use it?

| Benefit                          | Explanation                                                                                                                                  |
|----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| **Parallel client usage**        | Multiple LLM endpoints can be configured and used simultaneously, improving throughput.                                                      |
| **Automatic request scheduling** | Calls are enqueued and dispatched to the first free client, so you never have to manage locks yourself.                                      |
| **Simple synchronous API**       | Despite the internal concurrency, the public methods (`generate`, `chat`) block until a result is ready, making integration straightforward. |
| **Graceful shutdown**            | `close()` cleanly stops all background workers.                                                                                              |

#### Core API

```python
from pathlib import Path
from rdl_ml_utils.utils.openapi_queue_manager import OpenAPIQueue

# Initialise the queue with one or more client config files
queue = OpenAPIQueue([
    Path("configs/ollama-config.json"),
    Path("configs/ollama-config_lab4_1.json"),
])

# Generate a completion (handled by the first available client)
answer = queue.generate(
    "Explain quantum entanglement.",
    max_tokens=128,
)

# Chatâ€‘style interaction
reply = queue.chat(
    "What is the capital of France?",
    max_tokens=64,
)

# When finished, shut down the workers
queue.close()
```

The class handles all lowâ€‘level details (client selection, locking, task queuing)
so you can focus on the prompts and model logic.

---

### `training_handler.py`

The **Training handler** (`TrainingHandler`) streamlines dataset preparation for transformerâ€‘based models. It:

* Loads JSONâ€‘line datasets using the ðŸ¤— Datasets library.
* Instantiates a tokenizer from a Huggingâ€‘Face model (e.g., `bert-base-uncased`).
* Stores useful metadata such as the number of unique labels.
* Exposes readyâ€‘toâ€‘use `train_dataset` and `eval_dataset` attributes
  (creation of the actual `DataLoader`s is left to the user, keeping the class frameworkâ€‘agnostic).

#### Core API

```python
from rdl_ml_utils.handlers.training_handler import TrainingHandler

handler = TrainingHandler(
    train_dataset_file_path="data/train.jsonl",
    eval_dataset_file_path="data/valid.jsonl",
    base_model="distilbert-base-uncased",
    train_batch_size=16,
    workdir="./workdir",
)

# After initialization:
#   handler.tokenizer          -> AutoTokenizer instance
#   handler.train_dataset      -> ðŸ¤— Dataset with training examples
#   handler.eval_dataset       -> ðŸ¤— Dataset with validation examples
#   handler.uniq_labels        -> Set of label strings
```

#### What the class does internally

```python
# ... existing code ...

self.tokenizer = AutoTokenizer.from_pretrained(
    self.base_model, use_fast=True
)

data = load_dataset(
    "json",
    cache_dir="./cache",
    data_files={
        "train": self.train_dataset_file_path,
        "validation": self.eval_dataset_file_path,
    },
)

self.train_dataset = data["train"]
self.eval_dataset = data["validation"]

# ... existing code ...
```

The handler is deliberately lightweight: it only prepares raw datasets and tokenizers, leaving model definition,
optimizer setup and training loops to the userâ€™s own script or training framework (PyTorch, TensorFlow, ðŸ¤— Trainer,
etc.). This makes it easy to plug into existing pipelines while keeping reproducibility (datasets are cached under
`./cache`).

---

### `wandb_handler.py`

The **WandB handler** (`WanDBHandler`) centralises all interactions with the Weights & Biases service, providing a
highâ€‘level API for:

| Action                       | Method                                                                                | Description                                                                                                     |
|------------------------------|---------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **Initialize a run**         | `init_wandb`                                                                          | Sets up a W&B run with project name, tags, and a merged configuration dict (runâ€‘specific + training arguments). |
| **Log scalar metrics**       | `log_metrics`                                                                         | Sends a dictionary of metric name â†’ value pairs, optionally with a step number.                                 |
| **Store datasets / models**  | `add_dataset`, `add_model`                                                            | Creates a `wandb.Artifact` of type *dataset* or *model* and uploads the supplied directory.                     |
| **Finish a run**             | `finish_wandb`                                                                        | Calls `wandb.run.finish()` to close the run cleanly.                                                            |
| **Prepare run metadata**     | `prepare_run_tags`, `prepare_run_name_with_date`, `prepare_simple_run_name_with_date` | Helper functions that add host information, timestamps and model identifiers to run names/tags.                 |
| **Merge configs**            | `prepare_run_config`                                                                  | Combines a userâ€‘provided dict with attributes of a `training_args` object (e.g., from ðŸ¤—â€¯Trainer).              |
| **Plot confusion matrix**    | `plot_confusion_matrix`                                                               | Uses `wandb.plot.confusion_matrix` to visualise classification performance.                                     |
| **Log detailed predictions** | `store_prediction_results`                                                            | Creates a `wandb.Table` with raw text, true label, predicted label and optional perâ€‘class probabilities.        |

#### Example Usage

```python
from rdl_ml_utils.handlers.wandb_handler import WanDBHandler


# Simple config object (could be a dataclass or Namespace)
class WandbConfig:
    PROJECT_NAME = "ml-experiments"
    PROJECT_TAGS = ["nlp", "classification"]
    PREFIX_RUN = "run_"
    BASE_RUN_NAME = "experiment"


wandb_cfg = WandbConfig()
run_cfg = {"base_model": "distilbert-base-uncased", "learning_rate": 3e-5}
training_args = None  # could be an argparse.Namespace with many fields

# Initialise run (name will include timestamp and model name)
WanDBHandler.init_wandb(
    wandb_config=wandb_cfg,
    run_config=run_cfg,
    training_args=training_args,
    run_name=None,  # autoâ€‘generated
)

# Log some metrics during training
for epoch in range(3):
    # ... training logic ...
    WanDBHandler.log_metrics({"epoch": epoch, "accuracy": 0.87 + epoch * 0.01})

# After training, store the model artifact
WanDBHandler.add_model(name="distilbert-finetuned", local_path="./workdir/model")

# Finish the run
WanDBHandler.finish_wandb()
```

#### Plotting a Confusion Matrix

```python
# ground_truth and predictions are listâ€‘like, class_names is a list of label strings
WanDBHandler.plot_confusion_matrix(
    ground_truth=y_true,
    predictions=y_pred,
    class_names=["neg", "pos"],
    probs=prediction_probs,  # optional probability matrix
)
```

#### Storing Detailed Prediction Results

```python
WanDBHandler.store_prediction_results(
    texts_str=test_texts,
    ground_truth=y_true,
    pred_labels=y_pred,
    probs=prediction_probs,
)
```

All helper methods automatically add the host name to run tags, ensuring that runs from different machines are easily
distinguishable.

---

### `prompt_handler.py`

The **Prompt handler** (`PromptHandler`) offers a simple way to load, store, and retrieve textual prompts stored as
`*.prompt` files.  
Prompts are indexed by a *key* that corresponds to the fileâ€™s path **relative to the base directory**, using forward
slashes and **without the file extension**.

#### Core API

```python
from rdl_ml_utils.handlers.prompt_handler import PromptHandler

# Initialise the handler pointing at a directory that contains *.prompt files
prompt_dir = "configs/prompts"  # any directory you like
ph = PromptHandler(base_dir=prompt_dir)

# List all loaded prompts (key â†’ content)
all_prompts = ph.list_prompts()
print("Available prompts:", list(all_prompts.keys()))

# Retrieve a specific prompt
key = "system/default"  # corresponds to configs/prompts/system/default.prompt
prompt_text = ph.get_prompt(key)
print("Prompt content:", prompt_text)
```

#### How It Works

* **Recursive loading** â€“ The handler walks the `base_dir` recursively (`Path.rglob("*.prompt")`).
* **Key generation** â€“ For each file, the relative path (POSIX style) is taken, the `.prompt` suffix is stripped, and
  the result becomes the dictionary key.
* **Inâ€‘memory storage** â€“ Prompt contents are kept in a plain Python `dict[str, str]`, making subsequent lookâ€‘ups O(1).

#### Typical Useâ€‘Cases

| Scenario                     | How PromptHandler helps                                                                                                                                  |
|------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Prompt engineering**       | Keep a library of reusable prompts (system, fewâ€‘shot examples, taskâ€‘specific templates) in a dedicated folder; retrieve them by logical name at runtime. |
| **Dynamic prompt selection** | Based on experiment configuration, select the appropriate prompt key (`"qa/simple"`, `"summarization/long"` etc.) without hardâ€‘coding file paths.        |
| **Versioned prompts**        | Store multiple versions (`v1.prompt`, `v2.prompt`) in subâ€‘folders; the key reflects the version (`"summarization/v1"`).                                  |

#### Error handling

* `KeyError` is raised if a nonâ€‘existent key is requested.
* `RuntimeError` is raised if a prompt file cannot be read (e.g., permission issues).

---

## ðŸš€ Getting Started

1. **Clone the repository**

```shell script
git clone https://github.com/radlab-dev-group/ml-utils.git
cd ml-utils
```

2. **Create a virtual environment and install dependencies**

```shell script
python -m venv .venv
source .venv/bin/activate
pip install -e .
```

3. **Run the OpenAPI demo**

```shell script
python apps/openapi_test.py
```

---

## ðŸ“¦ Installation

```shell script
pip install git+https://github.com/radlab-dev-group/ml-utils.git
```

or, after cloning:

```shell script
pip install .
```

---

## ðŸ“œ License

This project is licensed under the Apache 2.0 License â€“ see the [LICENSE](LICENSE) file for details.